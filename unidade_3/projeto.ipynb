{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dataset.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Language'].fillna('', inplace=True)\n",
    "\n",
    "# Identificar as linhas em que o texto é uma foto\n",
    "mask_photo = dataset['Language'].astype(str).str.startswith(\"[Photo\")\n",
    "\n",
    "# Remover as linhas em que o texto é uma foto\n",
    "dataset = dataset[~mask_photo]\n",
    "\n",
    "\n",
    "# Certificar-se de lidar com valores NaN adequadamente se houver\n",
    "dataset.dropna(subset=['Language'], inplace=True)\n",
    "\n",
    "# Verificar se as linhas foram removidas\n",
    "print(dataset['Language'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar as linhas em que o texto é uma foto\n",
    "mask_photo = dataset['Language'].astype(str).str.startswith(\"[Video\")\n",
    "\n",
    "# Remover as linhas em que o texto é uma foto\n",
    "dataset = dataset[~mask_photo]\n",
    "\n",
    "# Certificar-se de lidar com valores NaN adequadamente se houver\n",
    "dataset.dropna(subset=['Language'], inplace=True)\n",
    "\n",
    "# Verificar se as linhas foram removidas\n",
    "print(dataset['Language'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Remove square brackets and contents inside\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(rf'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "    # Remove newline characters\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove words containing digits\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text'] = dataset['Text'].apply(lambda x:clean_text(x))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Language'] = dataset['Language'].apply(lambda x:clean_text(x))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if pd.notnull(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "        return ' '.join(filtered_tokens)\n",
    "    return text\n",
    "def lemmatize_text(text):\n",
    "    if pd.notnull(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(lemmatized_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of your code\n",
    "dataset['Text'] = dataset['Text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text'] = dataset['Text'].apply(lambda x: lemmatize_text(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import mlflow\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separar\n",
    "tweets = dataset['Text']\n",
    "language_labels = dataset['Language']\n",
    "sentiment_labels = dataset['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_train(mlflow_experiment_id):\n",
    "\n",
    "    tweets_train, tweets_test, lang_labels_train, lang_labels_test, sent_labels_train, sent_labels_test = train_test_split(\n",
    "            tweets, language_labels, sentiment_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "   \n",
    "\n",
    "    with mlflow.start_run(experiment_id=mlflow_experiment_id):\n",
    "        # classificacao de language\n",
    "        language_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LinearSVC())\n",
    "        ])\n",
    "        # Train the language  classification model\n",
    "        language_pipeline.fit(tweets_train, lang_labels_train)\n",
    "\n",
    "\n",
    "        lang_predictions = language_pipeline.predict(tweets_test)\n",
    "\n",
    "        # Avaliando o desempenho\n",
    "        lang_accuracy = accuracy_score(lang_labels_test, lang_predictions)\n",
    "        lang_report = classification_report(lang_labels_test, lang_predictions)\n",
    "\n",
    "        mlflow.log_metric(\"lang_accuracy\", lang_accuracy)\n",
    "        mlflow.log_metric(\"lang_report\", lang_report)\n",
    "\n",
    "\n",
    "        \n",
    "        # Define the pipeline for sentiment classification\n",
    "        sentiment_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', MultinomialNB())\n",
    "        ])\n",
    "        # Train the sentiment classification model\n",
    "        sentiment_pipeline.fit(tweets_train, sent_labels_train)\n",
    "\n",
    "        # Avaliar o modelo de identificação de sentimento\n",
    "        sent_predictions = sentiment_pipeline.predict(tweets_test)\n",
    "\n",
    "        sent_accuracy = accuracy_score(lang_labels_test, sent_predictions)\n",
    "        sent_report = classification_report(sent_labels_test, sent_predictions)\n",
    "\n",
    "        mlflow.log_metric(\"sent_accuracy\", sent_accuracy)\n",
    "        mlflow.log_metric(\"lang_report\", sent_report)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train, tweets_test, lang_labels_train, lang_labels_test, sent_labels_train, sent_labels_test = train_test_split(\n",
    "            tweets, language_labels, sentiment_labels, test_size=0.2, random_state=42\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_labels_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_labels_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_labels_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificacao de language\n",
    "language_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "# Train the language  classification model\n",
    "language_pipeline.fit(tweets_train, lang_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline for sentiment classification\n",
    "sentiment_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "# Train the sentiment classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the language of a tweet\n",
    "tweet = \"Predict the language of a tweet\"\n",
    "predicted_language = language_pipeline.predict([tweet])[0]\n",
    "print(\"Predicted language:\", predicted_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment of a tweet\n",
    "tweet = \"reasonable material\"\n",
    "predicted_sentiment = sentiment_pipeline.predict([tweet])[0]\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo de identificação de idioma\n",
    "lang_predictions = language_pipeline.predict(tweets_test)\n",
    "lang_report = classification_report(lang_labels_test, lang_predictions)\n",
    "print(\"Language identification report:\\n\", lang_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo de identificação de sentimento\n",
    "sent_predictions = sentiment_pipeline.predict(tweets_test)\n",
    "sent_report = classification_report(sent_labels_test, sent_predictions)\n",
    "print(\"Sentiment classification report:\\n\", sent_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
